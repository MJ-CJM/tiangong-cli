# 04 - æ¨¡å‹ä¸ Provider æ¶æ„

**é€‚ç”¨ç‰ˆæœ¬**: `0.6.0-nightly`
**Commit Hash**: `b347fa25e9133d410c4210e3825ace0cae5b4ecb`
**æ–‡æ¡£æ—¥æœŸ**: 2025-10-01

---

## ğŸ“Œ å½“å‰çŠ¶æ€

### ç°æœ‰å®ç°
- **é»˜è®¤ Provider**: Google Gemini API (`@google/genai` SDK)
- **æ”¯æŒæ¨¡å‹**:
  - `gemini-1.5-flash` (é»˜è®¤)
  - `gemini-1.5-pro`
  - `gemini-2.0-flash-exp`
  - å…¶ä»– Gemini ç³»åˆ—æ¨¡å‹

### å®éªŒæ€§åŠŸèƒ½
- **ModelRouterService**: å¤šæ¨¡å‹è·¯ç”±æ¡†æ¶ï¼ˆä»£ç å­˜åœ¨ä½†æœªå®Œå…¨æ¿€æ´»ï¼‰
- **é€šä¹‰åƒé—®é€‚é…å™¨**: éƒ¨åˆ†ä»£ç å·²å®ç°ä½†éœ€å®Œå–„

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### å½“å‰æ¶æ„ï¼ˆGemini å•ä¸€ Providerï¼‰

```mermaid
graph TB
    App[Application] --> Chat[geminiChat.ts]
    Chat --> Client[GeminiClient]
    Client --> SDK[@google/genai SDK]
    SDK --> API[Gemini API]

    style API fill:#f96,stroke:#333
```

### ç›®æ ‡æ¶æ„ï¼ˆå¤š Provider æ”¯æŒï¼‰

```mermaid
graph TB
    App[Application] --> Chat[geminiChat.ts]
    Chat --> Router[ModelRouterService]

    Router --> Gemini[GeminiAdapter]
    Router --> Qwen[QwenAdapter]
    Router --> OpenAI[OpenAIAdapter]
    Router --> Claude[ClaudeAdapter]

    Gemini --> GeminiAPI[Gemini API]
    Qwen --> QwenAPI[é€šä¹‰åƒé—® API]
    OpenAI --> OpenAIAPI[OpenAI API]
    Claude --> ClaudeAPI[Claude API]

    style Router fill:#9f9,stroke:#333,stroke-width:3px
```

---

## ğŸ”Œ ModelAdapter æ¥å£è®¾è®¡

### æ ¸å¿ƒæ¥å£

```typescript
// packages/core/src/routing/types.ts

export interface ModelAdapter {
  /**
   * æ¨¡å‹å”¯ä¸€æ ‡è¯†ç¬¦
   */
  readonly name: string;

  /**
   * éæµå¼ç”Ÿæˆ
   */
  generateContent(
    messages: Content[],
    options: GenerateContentConfig
  ): Promise<GenerateContentResponse>;

  /**
   * æµå¼ç”Ÿæˆ
   */
  generateContentStream(
    messages: Content[],
    options: GenerateContentConfig
  ): AsyncGenerator<GenerateContentResponse>;

  /**
   * å¥åº·æ£€æŸ¥
   */
  healthCheck(): Promise<boolean>;

  /**
   * è·å–æ¨¡å‹å…ƒæ•°æ®
   */
  getMetadata(): ModelMetadata;
}

export interface ModelMetadata {
  name: string;
  provider: string;
  contextWindow: number;
  maxOutputTokens: number;
  supportsFunctionCalling: boolean;
  supportsVision: boolean;
  pricing?: {
    inputPer1M: number;
    outputPer1M: number;
  };
}
```

### Gemini Adapter å®ç°ï¼ˆç°æœ‰ï¼‰

```typescript
// packages/core/src/routing/gemini-adapter.ts

export class GeminiAdapter implements ModelAdapter {
  constructor(
    private readonly client: GeminiClient,
    private readonly model: string
  ) {}

  async generateContent(
    messages: Content[],
    options: GenerateContentConfig
  ): Promise<GenerateContentResponse> {
    return this.client.generateContent(messages, options);
  }

  async *generateContentStream(
    messages: Content[],
    options: GenerateContentConfig
  ): AsyncGenerator<GenerateContentResponse> {
    const stream = await this.client.generateContentStream(messages, options);
    for await (const chunk of stream) {
      yield chunk;
    }
  }

  async healthCheck(): Promise<boolean> {
    try {
      await this.client.generateContent(
        [{ role: 'user', parts: [{ text: 'test' }] }],
        { maxOutputTokens: 1 }
      );
      return true;
    } catch {
      return false;
    }
  }

  getMetadata(): ModelMetadata {
    return {
      name: this.model,
      provider: 'google',
      contextWindow: 1_048_576, // 1M tokens
      maxOutputTokens: 8192,
      supportsFunctionCalling: true,
      supportsVision: true,
    };
  }
}
```

---

## ğŸš€ ModelRouterServiceï¼ˆè·¯ç”±å±‚ï¼‰

### æ ¸å¿ƒåŠŸèƒ½

```typescript
// packages/core/src/routing/modelRouterService.ts

export class ModelRouterService {
  private adapters: Map<string, ModelAdapter> = new Map();

  /**
   * æ³¨å†Œ Adapter
   */
  registerAdapter(adapter: ModelAdapter): void {
    this.adapters.set(adapter.name, adapter);
  }

  /**
   * æ ¹æ®æ¨¡å‹åç§°è·¯ç”±è¯·æ±‚
   */
  async routeRequest(
    modelName: string,
    messages: Content[],
    options: GenerateContentConfig,
    streaming: boolean = false
  ): Promise<GenerateContentResponse | AsyncGenerator> {
    const adapter = this.adapters.get(modelName);

    if (!adapter) {
      throw new Error(`Unknown model: ${modelName}`);
    }

    if (streaming) {
      return adapter.generateContentStream(messages, options);
    } else {
      return adapter.generateContent(messages, options);
    }
  }

  /**
   * Fallback æœºåˆ¶ï¼šä¸»æ¨¡å‹å¤±è´¥æ—¶å°è¯•å¤‡ç”¨æ¨¡å‹
   */
  async requestWithFallback(
    primaryModel: string,
    fallbackModels: string[],
    messages: Content[],
    options: GenerateContentConfig
  ): Promise<GenerateContentResponse> {
    const models = [primaryModel, ...fallbackModels];

    for (const model of models) {
      try {
        return await this.routeRequest(model, messages, options, false) as GenerateContentResponse;
      } catch (error) {
        console.warn(`Model ${model} failed, trying next...`);
        if (model === models[models.length - 1]) {
          throw error; // æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        }
      }
    }

    throw new Error('All models failed');
  }
}
```

---

## ğŸ”§ æ–°å¢ Provider å®ç°æŒ‡å—

### ç¤ºä¾‹ï¼šOpenAI Adapter

```typescript
// packages/core/src/routing/openai-adapter.ts

import OpenAI from 'openai';
import type { Content, GenerateContentResponse, Part } from '@google/genai';

export class OpenAIAdapter implements ModelAdapter {
  private client: OpenAI;

  constructor(apiKey: string, baseURL?: string) {
    this.client = new OpenAI({ apiKey, baseURL });
  }

  async generateContent(
    messages: Content[],
    options: GenerateContentConfig
  ): Promise<GenerateContentResponse> {
    // 1. è½¬æ¢æ ¼å¼ï¼šGemini Content â†’ OpenAI Messages
    const openaiMessages = this.convertToOpenAIMessages(messages);

    // 2. è½¬æ¢å·¥å…·å®šä¹‰
    const tools = this.convertTools(options.tools);

    // 3. è°ƒç”¨ OpenAI API
    const response = await this.client.chat.completions.create({
      model: 'gpt-4-turbo',
      messages: openaiMessages,
      tools: tools,
      max_tokens: options.maxOutputTokens,
      temperature: options.temperature,
    });

    // 4. è½¬æ¢å“åº”ï¼šOpenAI Response â†’ Gemini Response
    return this.convertToGeminiResponse(response);
  }

  async *generateContentStream(
    messages: Content[],
    options: GenerateContentConfig
  ): AsyncGenerator<GenerateContentResponse> {
    const stream = await this.client.chat.completions.create({
      model: 'gpt-4-turbo',
      messages: this.convertToOpenAIMessages(messages),
      stream: true,
    });

    for await (const chunk of stream) {
      yield this.convertChunkToGeminiResponse(chunk);
    }
  }

  // æ ¼å¼è½¬æ¢è¾…åŠ©æ–¹æ³•
  private convertToOpenAIMessages(contents: Content[]): OpenAI.ChatCompletionMessageParam[] {
    return contents.map(content => ({
      role: content.role === 'model' ? 'assistant' : content.role,
      content: this.extractText(content.parts),
    }));
  }

  private extractText(parts: Part[]): string {
    return parts
      .filter(p => 'text' in p)
      .map(p => (p as any).text)
      .join('');
  }

  private convertToGeminiResponse(openaiResp: any): GenerateContentResponse {
    return {
      candidates: [
        {
          content: {
            role: 'model',
            parts: [{ text: openaiResp.choices[0].message.content }],
          },
          finishReason: 'STOP',
          index: 0,
        },
      ],
      usageMetadata: {
        promptTokenCount: openaiResp.usage.prompt_tokens,
        candidatesTokenCount: openaiResp.usage.completion_tokens,
        totalTokenCount: openaiResp.usage.total_tokens,
      },
    };
  }

  async healthCheck(): Promise<boolean> {
    try {
      await this.client.models.retrieve('gpt-4-turbo');
      return true;
    } catch {
      return false;
    }
  }

  getMetadata(): ModelMetadata {
    return {
      name: 'gpt-4-turbo',
      provider: 'openai',
      contextWindow: 128000,
      maxOutputTokens: 4096,
      supportsFunctionCalling: true,
      supportsVision: true,
      pricing: {
        inputPer1M: 10.0,
        outputPer1M: 30.0,
      },
    };
  }
}
```

---

## ğŸ” é‡è¯•ä¸è¶…æ—¶ç­–ç•¥

### å½“å‰å®ç°

```typescript
// packages/core/src/utils/retry.ts

export async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  options: {
    maxAttempts: number;
    initialDelayMs: number;
    backoffFactor?: number; // é»˜è®¤ 2ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
  }
): Promise<T> {
  let lastError: Error;

  for (let attempt = 0; attempt < options.maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      if (attempt === options.maxAttempts - 1) {
        throw lastError;
      }

      const delay = options.initialDelayMs * Math.pow(options.backoffFactor ?? 2, attempt);
      await sleep(delay);
    }
  }

  throw lastError!;
}
```

### é…ç½®é‡è¯•å‚æ•°

```typescript
// åœ¨ Config ä¸­æ·»åŠ é‡è¯•é…ç½®
export interface RetryConfig {
  maxAttempts: number;
  initialDelayMs: number;
  backoffFactor: number;
  retryableErrors: string[]; // åªé‡è¯•ç‰¹å®šé”™è¯¯
}

const defaultRetryConfig: RetryConfig = {
  maxAttempts: 3,
  initialDelayMs: 500,
  backoffFactor: 2,
  retryableErrors: [
    'RATE_LIMIT_EXCEEDED',
    'INTERNAL_ERROR',
    'DEADLINE_EXCEEDED',
  ],
};
```

---

## ğŸŒ Fallback æœºåˆ¶

### å½“å‰å®ç°

```typescript
// packages/core/src/fallback/handler.ts

export async function handleFallback(
  config: Config,
  primaryError: Error,
  messages: Content[],
  options: GenerateContentConfig
): Promise<GenerateContentResponse> {
  const fallbackModels = config.getFallbackModels();

  for (const model of fallbackModels) {
    try {
      console.log(`Trying fallback model: ${model}`);
      return await generateWithModel(model, messages, options);
    } catch (error) {
      console.warn(`Fallback model ${model} also failed`);
    }
  }

  throw new Error(`All models failed. Original error: ${primaryError.message}`);
}
```

### é…ç½® Fallback

```json
{
  "model": "gemini-2.0-flash",
  "fallbackModels": [
    "gemini-1.5-flash",
    "gemini-1.5-pro"
  ]
}
```

---

## ğŸ” æ¨¡å‹é€‰æ‹©ç­–ç•¥

### æŒ‰ä»»åŠ¡ç±»å‹é€‰æ‹©

```typescript
export function selectModelForTask(task: TaskType, config: Config): string {
  const modelMapping: Record<TaskType, string> = {
    'code-generation': 'gemini-2.0-flash',
    'long-context': 'gemini-1.5-pro',
    'fast-response': 'gemini-1.5-flash',
    'vision': 'gemini-1.5-pro',
  };

  return modelMapping[task] || config.getDefaultModel();
}
```

### æŒ‰æˆæœ¬ä¼˜åŒ–

```typescript
export function selectCheapestModel(config: Config): string {
  const models = config.getAvailableModels();
  return models.reduce((cheapest, current) => {
    const cheapestPrice = cheapest.pricing.inputPer1M + cheapest.pricing.outputPer1M;
    const currentPrice = current.pricing.inputPer1M + current.pricing.outputPer1M;
    return currentPrice < cheapestPrice ? current : cheapest;
  }).name;
}
```

---

## ğŸ“Š å¯è§‚æµ‹æ€§

### è¯·æ±‚æ—¥å¿—

```typescript
export async function loggedGenerateContent(
  adapter: ModelAdapter,
  messages: Content[],
  options: GenerateContentConfig
): Promise<GenerateContentResponse> {
  const startTime = Date.now();

  try {
    const response = await adapter.generateContent(messages, options);
    const duration = Date.now() - startTime;

    // è®°å½•æˆåŠŸè¯·æ±‚
    telemetry.log({
      event: 'model_request_success',
      model: adapter.name,
      duration,
      inputTokens: response.usageMetadata?.promptTokenCount,
      outputTokens: response.usageMetadata?.candidatesTokenCount,
    });

    return response;
  } catch (error) {
    // è®°å½•å¤±è´¥è¯·æ±‚
    telemetry.log({
      event: 'model_request_failure',
      model: adapter.name,
      duration: Date.now() - startTime,
      error: (error as Error).message,
    });

    throw error;
  }
}
```

---

## ğŸ¯ æœ€å°å¯è¡Œå®ç°ï¼šé€šä¹‰åƒé—®é€‚é…å™¨

### å®ç°ç›®æ ‡
- æ”¯æŒé€šä¹‰åƒé—® Qwen-Coder æ¨¡å‹
- å®ç°åŸºç¡€å¯¹è¯åŠŸèƒ½ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰
- é›†æˆåˆ° ModelRouterService

### ä»£ç å®ç°

```typescript
// packages/core/src/routing/qwen-adapter.ts

import type { Content, GenerateContentResponse } from '@google/genai';

export class QwenAdapter implements ModelAdapter {
  readonly name = 'qwen-coder-turbo';
  private readonly baseURL: string;
  private readonly apiKey: string;

  constructor(config: { apiKey: string; baseURL?: string }) {
    this.apiKey = config.apiKey;
    this.baseURL = config.baseURL || 'https://dashscope.aliyuncs.com/compatible-mode/v1';
  }

  async generateContent(
    messages: Content[],
    options: GenerateContentConfig
  ): Promise<GenerateContentResponse> {
    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`,
      },
      body: JSON.stringify({
        model: 'qwen-coder-turbo',
        messages: this.convertMessages(messages),
        max_tokens: options.maxOutputTokens,
        temperature: options.temperature || 0.7,
      }),
    });

    if (!response.ok) {
      throw new Error(`Qwen API error: ${response.statusText}`);
    }

    const data = await response.json();
    return this.convertResponse(data);
  }

  async *generateContentStream(
    messages: Content[],
    options: GenerateContentConfig
  ): AsyncGenerator<GenerateContentResponse> {
    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`,
      },
      body: JSON.stringify({
        model: 'qwen-coder-turbo',
        messages: this.convertMessages(messages),
        stream: true,
      }),
    });

    const reader = response.body!.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(line => line.startsWith('data: '));

      for (const line of lines) {
        if (line.includes('[DONE]')) continue;
        const data = JSON.parse(line.slice(6));
        yield this.convertStreamChunk(data);
      }
    }
  }

  private convertMessages(contents: Content[]): any[] {
    return contents.map(c => ({
      role: c.role === 'model' ? 'assistant' : c.role,
      content: c.parts.map(p => (p as any).text).join(''),
    }));
  }

  private convertResponse(qwenResp: any): GenerateContentResponse {
    return {
      candidates: [
        {
          content: {
            role: 'model',
            parts: [{ text: qwenResp.choices[0].message.content }],
          },
          finishReason: 'STOP',
          index: 0,
        },
      ],
      usageMetadata: {
        promptTokenCount: qwenResp.usage?.prompt_tokens || 0,
        candidatesTokenCount: qwenResp.usage?.completion_tokens || 0,
        totalTokenCount: qwenResp.usage?.total_tokens || 0,
      },
    };
  }

  private convertStreamChunk(chunk: any): GenerateContentResponse {
    return {
      candidates: [
        {
          content: {
            role: 'model',
            parts: [{ text: chunk.choices[0].delta.content || '' }],
          },
          finishReason: chunk.choices[0].finish_reason === 'stop' ? 'STOP' : 'ONGOING',
          index: 0,
        },
      ],
    };
  }

  async healthCheck(): Promise<boolean> {
    try {
      await this.generateContent(
        [{ role: 'user', parts: [{ text: 'hi' }] }],
        { maxOutputTokens: 1 }
      );
      return true;
    } catch {
      return false;
    }
  }

  getMetadata(): ModelMetadata {
    return {
      name: 'qwen-coder-turbo',
      provider: 'qwen',
      contextWindow: 8192,
      maxOutputTokens: 2048,
      supportsFunctionCalling: false,
      supportsVision: false,
    };
  }
}
```

---

**ä¸‹ä¸€æ­¥**: é˜…è¯» [05-extensibility.md](./05-extensibility.md) äº†è§£æ‰©å±•æ–¹æ¡ˆã€‚
